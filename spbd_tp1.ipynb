{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diogocristovao/SPBD_tp1/blob/main/spbd_tp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OT3ms58h5G7Y"
      },
      "outputs": [],
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "\n",
        "!wget -q -O energy-readings.csv https://raw.githubusercontent.com/smduarte/spbd-2425/refs/heads/main/docs/labs/projs/energy-readings.csv\n",
        "!head -10 energy-readings.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X3HBc8F8LOh",
        "outputId": "b454cc46-bf34-4d98-dcdc-c1e12cf5125b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date;sensor;energy\n",
            "2024-02-01 00:00:00;D;2615.0\n",
            "2024-02-01 00:00:18;C;1098.8\n",
            "2024-02-01 00:00:25;A;650.5\n",
            "2024-02-01 00:00:33;J;966.7\n",
            "2024-02-01 00:00:42;H;2145.4\n",
            "2024-02-01 00:00:54;E;1874.0\n",
            "2024-02-01 00:01:52;K;841.2\n",
            "2024-02-01 00:02:00;E;1874.1\n",
            "2024-02-01 00:02:20;I;927.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('energy').getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    readings = spark.read.csv('energy-readings.csv',\n",
        "                             sep =';', header=True, inferSchema=True)\n",
        "\n",
        "    readings.printSchema()\n",
        "\n",
        "\n",
        "    readings.show(11)\n",
        "except Exception as err:\n",
        "    print(err)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK9Kh1B95PNn",
        "outputId": "95189a0c-d9da-4413-c5b0-ac8fb35fd9fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- sensor: string (nullable = true)\n",
            " |-- energy: double (nullable = true)\n",
            "\n",
            "+-------------------+------+------+\n",
            "|               date|sensor|energy|\n",
            "+-------------------+------+------+\n",
            "|2024-02-01 00:00:00|     D|2615.0|\n",
            "|2024-02-01 00:00:18|     C|1098.8|\n",
            "|2024-02-01 00:00:25|     A| 650.5|\n",
            "|2024-02-01 00:00:33|     J| 966.7|\n",
            "|2024-02-01 00:00:42|     H|2145.4|\n",
            "|2024-02-01 00:00:54|     E|1874.0|\n",
            "|2024-02-01 00:01:52|     K| 841.2|\n",
            "|2024-02-01 00:02:00|     E|1874.1|\n",
            "|2024-02-01 00:02:20|     I| 927.2|\n",
            "|2024-02-01 00:02:36|     K| 841.3|\n",
            "|2024-02-01 00:03:24|     G| 833.7|\n",
            "+-------------------+------+------+\n",
            "only showing top 11 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Configura a Spark Session\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "                    .appName('energy').getOrCreate()\n",
        "\n",
        "# Carrega o arquivo CSV\n",
        "try:\n",
        "    readings = spark.read.csv('energy-readings.csv',\n",
        "                              sep=';', header=True, inferSchema=True)\n",
        "\n",
        "    # Imprime o esquema dos dados\n",
        "    readings.printSchema()\n",
        "\n",
        "    # Filtra apenas os dados de fevereiro de 2024\n",
        "    readings_february = readings.filter((year(\"date\") == 2024) & (month(\"date\") == 2))\n",
        "\n",
        "    # Calcula o valor máximo e mínimo de energia para cada sensor no mês de fevereiro\n",
        "    energy_per_sensor = readings_february.groupBy(\"sensor\").agg(\n",
        "        max(\"energy\").alias(\"max_energy\"),\n",
        "        min(\"energy\").alias(\"min_energy\")\n",
        "    )\n",
        "\n",
        "    # Exibe o valor máximo e mínimo de energia de cada sensor no mês de fevereiro\n",
        "    energy_per_sensor.show()\n",
        "\n",
        "except Exception as err:\n",
        "   print(err)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lXLG-peP1tt",
        "outputId": "e568faa7-815d-40c4-cde1-b7d698b3b2cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- sensor: string (nullable = true)\n",
            " |-- energy: double (nullable = true)\n",
            "\n",
            "+------+----------+----------+\n",
            "|sensor|max_energy|min_energy|\n",
            "+------+----------+----------+\n",
            "|     K|    1067.7|     841.2|\n",
            "|     F|    908.41|     748.0|\n",
            "|     E|   2322.76|    1874.0|\n",
            "|     B|    757.31|     627.5|\n",
            "|     D|    3102.4|    2615.0|\n",
            "|     C|   1356.02|    1098.8|\n",
            "|     J|   1197.55|     966.7|\n",
            "|     A|    816.88|     650.5|\n",
            "|     G|   1002.17|     833.7|\n",
            "|     I|   1278.61|     927.2|\n",
            "|     H|    2625.0|    2145.4|\n",
            "+------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Alinea a)\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Configura a Spark Session\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "                    .appName('energy').getOrCreate()\n",
        "\n",
        "# Carrega o arquivo CSV\n",
        "try:\n",
        "    readings = spark.read.csv('energy-readings.csv',\n",
        "                              sep=';', header=True, inferSchema=True)\n",
        "\n",
        "    # Imprime o esquema dos dados\n",
        "    readings.printSchema()\n",
        "\n",
        "    # Filtra apenas os dados de fevereiro de 2024\n",
        "    readings = readings.filter((year(\"date\") == 2024) & (month(\"date\") == 2))\n",
        "\n",
        "    # Para cada sensor, queremos encontrar o valor de energia inicial e final em fevereiro\n",
        "    # Usa o primeiro e o último valor de energia de cada sensor para calcular o consumo total\n",
        "    total_energy = readings.groupBy(\"sensor\").agg(\n",
        "        (max(\"energy\") - min(\"energy\")).alias(\"total_energy_consumed\")\n",
        "    )\n",
        "\n",
        "     # Calcula a soma da energia total gasta por todos os sensores\n",
        "    total_energy_sum = total_energy.agg(sum(\"total_energy_consumed\").alias(\"total_energy_all_sensors\")).collect()[0][\"total_energy_all_sensors\"]\n",
        "\n",
        "\n",
        "    # Exibe o consumo total de energia de cada sensor\n",
        "    total_energy.show()\n",
        "\n",
        "     # Exibe a soma da energia total gasta pelos 11 sensores\n",
        "    print(\"Total energy consumed by all sensors:\", total_energy_sum)\n",
        "\n",
        "\n",
        "except Exception as err:\n",
        "   print(err)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KFXxYGYAGnH",
        "outputId": "e9fa1cfc-0c3a-4731-f699-1ab1d5ae41ba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- sensor: string (nullable = true)\n",
            " |-- energy: double (nullable = true)\n",
            "\n",
            "+------+---------------------+\n",
            "|sensor|total_energy_consumed|\n",
            "+------+---------------------+\n",
            "|     K|                226.5|\n",
            "|     F|   160.40999999999997|\n",
            "|     E|    448.7600000000002|\n",
            "|     B|   129.80999999999995|\n",
            "|     D|    487.4000000000001|\n",
            "|     C|               257.22|\n",
            "|     J|    230.8499999999999|\n",
            "|     A|               166.38|\n",
            "|     G|    168.4699999999999|\n",
            "|     I|   351.40999999999985|\n",
            "|     H|    479.5999999999999|\n",
            "+------+---------------------+\n",
            "\n",
            "Total energy consumed by all sensors: 3106.8099999999995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Alinea b)\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Configura a Spark Session\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "                    .appName('energy').getOrCreate()\n",
        "\n",
        "# Carrega o arquivo CSV\n",
        "try:\n",
        "    readings = spark.read.csv('energy-readings.csv',\n",
        "                              sep=';', header=True, inferSchema=True)\n",
        "\n",
        "    # Imprime o esquema dos dados\n",
        "    readings.printSchema()\n",
        "\n",
        "    # Filtra apenas os dados de fevereiro de 2024\n",
        "    readings = readings.filter((year(\"date\") == 2024) & (month(\"date\") == 2))\n",
        "\n",
        "    # Extrai a data (sem o tempo) para agrupar por dia\n",
        "    readings = readings.withColumn(\"date\", to_date(\"date\"))\n",
        "\n",
        "    # Usa Window Functions para pegar a última leitura de cada sensor por dia\n",
        "    window_spec = Window.partitionBy(\"sensor\", \"date\").orderBy(desc(\"date\"))\n",
        "    daily_last_reading = readings.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
        "                                 .filter(col(\"row_number\") == 1) \\\n",
        "                                 .drop(\"row_number\")\n",
        "\n",
        "    # Calcula o total acumulado diário de energia consumida por sensor\n",
        "    daily_running_total = daily_last_reading.groupBy(\"date\").agg(\n",
        "        sum(\"energy\").alias(\"running_total_energy\")\n",
        "    ).orderBy(\"date\")\n",
        "\n",
        "    # Exibe o total acumulado de energia consumida por dia\n",
        "    daily_running_total.show(truncate=False)\n",
        "\n",
        "except Exception as err:\n",
        "   print(err)"
      ],
      "metadata": {
        "id": "GTbjymauxVMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a1e349-da38-4f00-edc8-73f00ed1df9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- sensor: string (nullable = true)\n",
            " |-- energy: double (nullable = true)\n",
            "\n",
            "+----------+--------------------+\n",
            "|date      |running_total_energy|\n",
            "+----------+--------------------+\n",
            "|2024-02-01|13328.000000000002  |\n",
            "|2024-02-02|13448.300000000001  |\n",
            "|2024-02-09|14377.2             |\n",
            "|2024-02-10|14433.5             |\n",
            "|2024-02-11|14547.599999999997  |\n",
            "|2024-02-12|14665.599999999999  |\n",
            "|2024-02-13|14776.300000000001  |\n",
            "|2024-02-14|14889.299999999997  |\n",
            "|2024-02-15|14982.4             |\n",
            "|2024-02-16|15063.799999999997  |\n",
            "|2024-02-18|15293.6             |\n",
            "|2024-02-19|15351.599999999999  |\n",
            "|2024-02-20|15431.400000000001  |\n",
            "|2024-02-21|15515.4             |\n",
            "|2024-02-22|15598.5             |\n",
            "|2024-02-23|15675.400000000001  |\n",
            "|2024-02-24|15839.800000000001  |\n",
            "|2024-02-25|15903.369999999997  |\n",
            "|2024-02-26|16003.189999999997  |\n",
            "|2024-02-27|16095.89            |\n",
            "+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "For each sensor, separately:\n",
        "Compute the total energy consumed and the average energy consumption per day.\n"
      ],
      "metadata": {
        "id": "Z6TKNnOqM6k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Alinea c)\n",
        "\n",
        "# Configura a Spark Session\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "                    .appName('energy').getOrCreate()\n",
        "\n",
        "# Carrega o arquivo CSV\n",
        "try:\n",
        "    readings = spark.read.csv('energy-readings.csv',\n",
        "                              sep=';', header=True, inferSchema=True)\n",
        "\n",
        "    # Imprime o esquema dos dados\n",
        "    readings.printSchema()\n",
        "\n",
        "    # Filtra apenas os dados de fevereiro de 2024\n",
        "    readings = readings.filter((year(\"date\") == 2024) & (month(\"date\") == 2))\n",
        "\n",
        "    # Calcula o consumo total de energia para cada sensor\n",
        "    total_energy = readings.groupBy(\"sensor\").agg(\n",
        "        (max(\"energy\") - min(\"energy\")).alias(\"total_energy_consumed\")\n",
        "    )\n",
        "\n",
        "    # Calcula o número de dias de leitura em fevereiro para cada sensor\n",
        "    days_count = readings.groupBy(\"sensor\").agg(countDistinct(\"date\").alias(\"days_count\"))\n",
        "\n",
        "    # Junta os dados de total de energia e de contagem de dias\n",
        "    total_energy = total_energy.join(days_count, on=\"sensor\")\n",
        "\n",
        "    # Calcula o consumo médio de energia por dia para cada sensor\n",
        "    total_energy = total_energy.withColumn(\n",
        "        \"average_daily_energy_consumed\", col(\"total_energy_consumed\") / col(\"days_count\")\n",
        "    )\n",
        "\n",
        "    # Exibe o consumo total de energia e o consumo médio diário de cada sensor\n",
        "    total_energy.select(\"sensor\", \"total_energy_consumed\", \"average_daily_energy_consumed\").show()\n",
        "\n",
        "except Exception as err:\n",
        "   print(err)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvmfervkNEyA",
        "outputId": "db5ca4aa-6155-43ef-8880-ceda9fc6cbb5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- sensor: string (nullable = true)\n",
            " |-- energy: double (nullable = true)\n",
            "\n",
            "+------+---------------------+-----------------------------+\n",
            "|sensor|total_energy_consumed|average_daily_energy_consumed|\n",
            "+------+---------------------+-----------------------------+\n",
            "|     K|                226.5|          0.01350143061516452|\n",
            "|     F|   160.40999999999997|         0.009727713765918737|\n",
            "|     E|    448.7600000000002|          0.02545433919455475|\n",
            "|     B|   129.80999999999995|         0.007758651604805448|\n",
            "|     D|    487.4000000000001|          0.02851292851292852|\n",
            "|     C|               257.22|         0.014900938477580814|\n",
            "|     J|    230.8499999999999|         0.013889891696750896|\n",
            "|     A|               166.38|          0.00989709119029207|\n",
            "|     G|    168.4699999999999|         0.010093463543226884|\n",
            "|     I|   351.40999999999985|          0.02112854737854737|\n",
            "|     H|    479.5999999999999|          0.02783678681293168|\n",
            "+------+---------------------+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each sensor, separately:\n",
        "\n",
        "Compute the day of the month with minimum and maximum energy consumption."
      ],
      "metadata": {
        "id": "qnwenN2RRpoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Configura a Spark Session\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "                    .appName('energy').getOrCreate()\n",
        "\n",
        "# Carrega o arquivo CSV\n",
        "try:\n",
        "    readings = spark.read.csv('energy-readings.csv',\n",
        "                              sep=';', header=True, inferSchema=True)\n",
        "\n",
        "    # Imprime o esquema dos dados\n",
        "    readings.printSchema()\n",
        "\n",
        "    # Filtra apenas os dados de fevereiro de 2024\n",
        "    readings_february = readings.filter((year(\"date\") == 2024) & (month(\"date\") == 2))\n",
        "\n",
        "    # Calcula o consumo diário de energia para cada sensor\n",
        "    daily_energy = readings_february.groupBy(\"sensor\", \"date\").agg(\n",
        "        (max(\"energy\") - min(\"energy\")).alias(\"daily_energy_consumed\")\n",
        "    )\n",
        "\n",
        "    # Encontra o valor mínimo e máximo de consumo diário para cada sensor\n",
        "    min_energy_per_sensor = daily_energy.groupBy(\"sensor\").agg(\n",
        "        min(\"daily_energy_consumed\").alias(\"min_daily_energy\")\n",
        "    )\n",
        "\n",
        "    max_energy_per_sensor = daily_energy.groupBy(\"sensor\").agg(\n",
        "        max(\"daily_energy_consumed\").alias(\"max_daily_energy\")\n",
        "    )\n",
        "\n",
        "    # Faz o join para encontrar o dia correspondente ao consumo mínimo de energia para cada sensor\n",
        "    min_energy_day = min_energy_per_sensor.join(\n",
        "        daily_energy,\n",
        "        (daily_energy[\"sensor\"] == min_energy_per_sensor[\"sensor\"]) &\n",
        "        (daily_energy[\"daily_energy_consumed\"] == min_energy_per_sensor[\"min_daily_energy\"])\n",
        "    ).select(daily_energy[\"sensor\"], daily_energy[\"date\"].alias(\"min_energy_date\"), \"min_daily_energy\")\n",
        "\n",
        "    # Faz o join para encontrar o dia correspondente ao consumo máximo de energia para cada sensor\n",
        "    max_energy_day = max_energy_per_sensor.join(\n",
        "        daily_energy,\n",
        "        (daily_energy[\"sensor\"] == max_energy_per_sensor[\"sensor\"]) &\n",
        "        (daily_energy[\"daily_energy_consumed\"] == max_energy_per_sensor[\"max_daily_energy\"])\n",
        "    ).select(daily_energy[\"sensor\"], daily_energy[\"date\"].alias(\"max_energy_date\"), \"max_daily_energy\")\n",
        "\n",
        "    # Exibe o dia de consumo mínimo de energia para cada sensor\n",
        "    print(\"Dia com consumo mínimo de energia para cada sensor:\")\n",
        "    min_energy_day.show()\n",
        "\n",
        "    # Exibe o dia de consumo máximo de energia para cada sensor\n",
        "    print(\"Dia com consumo máximo de energia para cada sensor:\")\n",
        "    max_energy_day.show()\n",
        "\n",
        "except Exception as err:\n",
        "    print(err)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHk3G9VZRu3D",
        "outputId": "17210548-1e1b-4092-e27f-7b87c92184f1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- sensor: string (nullable = true)\n",
            " |-- energy: double (nullable = true)\n",
            "\n",
            "Column date#643, sensor#644 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.\n"
          ]
        }
      ]
    }
  ]
}