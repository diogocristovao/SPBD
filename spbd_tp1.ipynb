{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT3ms58h5G7Y"
      },
      "outputs": [],
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "\n",
        "!wget -q -O energy-readings.csv https://raw.githubusercontent.com/smduarte/spbd-2425/refs/heads/main/docs/labs/projs/energy-readings.csv\n",
        "!head -10 energy-readings.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X3HBc8F8LOh",
        "outputId": "93adc10e-65cd-4e35-9530-e628c8a53643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date;sensor;energy\n",
            "2024-02-01 00:00:00;D;2615.0\n",
            "2024-02-01 00:00:18;C;1098.8\n",
            "2024-02-01 00:00:25;A;650.5\n",
            "2024-02-01 00:00:33;J;966.7\n",
            "2024-02-01 00:00:42;H;2145.4\n",
            "2024-02-01 00:00:54;E;1874.0\n",
            "2024-02-01 00:01:52;K;841.2\n",
            "2024-02-01 00:02:00;E;1874.1\n",
            "2024-02-01 00:02:20;I;927.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('energy').getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "try :\n",
        "    readings = spark.read.csv('energy-readings.csv',\n",
        "                             sep =';', header=True, inferSchema=True)\n",
        "\n",
        "    readings.printSchema()\n",
        "\n",
        "\n",
        "    readings.show(5)\n",
        "except Exception as err:\n",
        "    print(err)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK9Kh1B95PNn",
        "outputId": "087392f0-e46a-4a1f-a4a1-c58c0481835b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- sensor: string (nullable = true)\n",
            " |-- energy: double (nullable = true)\n",
            "\n",
            "+-------------------+------+------+\n",
            "|               date|sensor|energy|\n",
            "+-------------------+------+------+\n",
            "|2024-02-01 00:00:00|     D|2615.0|\n",
            "|2024-02-01 00:00:18|     C|1098.8|\n",
            "|2024-02-01 00:00:25|     A| 650.5|\n",
            "|2024-02-01 00:00:33|     J| 966.7|\n",
            "|2024-02-01 00:00:42|     H|2145.4|\n",
            "+-------------------+------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Alinea a)\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Configura a Spark Session\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "                    .appName('energy').getOrCreate()\n",
        "\n",
        "# Carrega o arquivo CSV\n",
        "try:\n",
        "    readings = spark.read.csv('energy-readings.csv',\n",
        "                              sep=';', header=True, inferSchema=True)\n",
        "\n",
        "    # Imprime o esquema dos dados\n",
        "    readings.printSchema()\n",
        "\n",
        "    # Filtra apenas os dados de fevereiro de 2024\n",
        "    readings = readings.filter((year(\"timestamp\") == 2024) & (month(\"timestamp\") == 2))\n",
        "\n",
        "    # Para cada sensor, queremos encontrar o valor de energia inicial e final em fevereiro\n",
        "    # Usa o primeiro e o último valor de energia de cada sensor para calcular o consumo total\n",
        "    total_energy = readings.groupBy(\"sensor_id\").agg(\n",
        "        (max(\"energy\") - min(\"energy\")).alias(\"total_energy_consumed\")\n",
        "    )\n",
        "\n",
        "    # Exibe o consumo total de energia de cada sensor\n",
        "    total_energy.show()\n",
        "\n",
        "except Exception as err:\n",
        "   print(err)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KFXxYGYAGnH",
        "outputId": "37f8c06a-b531-45cf-bab0-bbb194be0923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- sensor: string (nullable = true)\n",
            " |-- energy: double (nullable = true)\n",
            "\n",
            "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `timestamp` cannot be resolved. Did you mean one of the following? [`date`, `energy`, `sensor`].;\n",
            "'Filter ((year('timestamp) = 2024) AND (month('timestamp) = 2))\n",
            "+- Relation [date#57,sensor#58,energy#59] csv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Alinea b)\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Configura a Spark Session\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "                    .appName('energy').getOrCreate()\n",
        "\n",
        "# Carrega o arquivo CSV\n",
        "try:\n",
        "    readings = spark.read.csv('energy-readings.csv',\n",
        "                              sep=';', header=True, inferSchema=True)\n",
        "\n",
        "    # Imprime o esquema dos dados\n",
        "    readings.printSchema()\n",
        "\n",
        "    # Filtra apenas os dados de fevereiro de 2024\n",
        "    readings = readings.filter((year(\"date\") == 2024) & (month(\"date\") == 2))\n",
        "\n",
        "    # Extrai a data (sem o tempo) para agrupar por dia\n",
        "    readings = readings.withColumn(\"date\", to_date(\"date\"))\n",
        "\n",
        "    # Usa Window Functions para pegar a última leitura de cada sensor por dia\n",
        "    window_spec = Window.partitionBy(\"sensor\", \"date\").orderBy(desc(\"date\"))\n",
        "    daily_last_reading = readings.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
        "                                 .filter(col(\"row_number\") == 1) \\\n",
        "                                 .drop(\"row_number\")\n",
        "\n",
        "    # Calcula o total acumulado diário de energia consumida por sensor\n",
        "    daily_running_total = daily_last_reading.groupBy(\"date\").agg(\n",
        "        sum(\"energy\").alias(\"running_total_energy\")\n",
        "    ).orderBy(\"date\")\n",
        "\n",
        "    # Exibe o total acumulado de energia consumida por dia\n",
        "    daily_running_total.show(truncate=False)\n",
        "\n",
        "except Exception as err:\n",
        "    print(err)"
      ],
      "metadata": {
        "id": "GTbjymauxVMi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}