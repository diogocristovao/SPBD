{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diogocristovao/SPBD/blob/main/docs/labs/projs/spbd2425_tp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Sistemas para Processamento de Big Data\n",
        "## TP2 - Energy Meter Live Monitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sensor data corresponds to regular readings from 11 residential energy meters. The data covers the month of February 2024.\n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "timestamp | sensor_id | energy\n",
        "----------|-------------|-----------\n",
        "timestamp | string  | float\n",
        "\n",
        "Each energy value (KWh) corresponds to the accumulated value of the meter at the time of measurement. As such,\n",
        "each meter is expected to produce a monotonically increasing series of pairs of timestamp and energy consummed up to that moment.\n",
        "\n",
        "The meters do not start at zero or at the same value.\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "For all the sensors combined:\n",
        "\n",
        "1. For the current month and current day, compute the running total energy consumed so far. The values should be updated every 5 minutes.\n",
        "\n",
        "2. For the current month and current day, compute the running total energy consumed so far, **as a percentage**, **compared to the same periods in February 2024**. The values should be updated every 5 minutes.\n",
        "\n",
        "For each sensor, separately:\n",
        "\n",
        "3. For the current month and current day, compute the running total energy consumed so far, as a percentage, **comparing the value of each individual sensor, relative to the same results for all the sensors together (as in #1)**. The values should be updated every 5 minutes. (Sorted in descending order by value and sensor.)\n",
        "\n",
        "**Note:** For simplicity, it is fine to assume the first reading of each day can be used to start counting how much energy has been consumed so far. There is no need to interpolate/estimate the value of the meters at midnight.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "Solve each question using Structured Spark Streaming."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Grading Criteria\n",
        "\n",
        "+ Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "\n",
        "December 6.\n",
        "\n",
        "Penalty of 0.25 grade points per day late.\n",
        "\n",
        "Penalty accumulates until the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark --quiet"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Archived February Energy Readings\n",
        "!wget -q -O /tmp/readings.csv https://raw.githubusercontent.com/smduarte/spbd-2425/refs/heads/main/docs/labs/projs/energy-readings.csv\n",
        "!grep \"2024-02\" /tmp/readings.csv > february-energy-readings.csv\n",
        "!head -2 february-energy-readings.csv\n"
      ],
      "metadata": {
        "id": "w38DVs9wBhhu",
        "outputId": "2cab388f-7b02-407f-d202-c3de5f9b6e7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-01 00:00:00;D;2615.0\n",
            "2024-02-01 00:00:18;C;1098.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start the Structured Source\n",
        "!wget -q -O - https://github.com/smduarte/spbd-2425/raw/main/scripts/json_energy_sender.tgz  | tar xfz - 2> /dev/null\n",
        "\n",
        "!nohup python json_energy_sender/server.py --filename json_energy_sender/energy-readings.csv --speedup 60 > /dev/null 2> /dev/null &"
      ],
      "metadata": {
        "id": "n4fbaIE0vedz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note: --speedup 60, means the stream is played 60x faster than realtime. Therefore, 1 second in real time corresponds to 1 minute of stream data.\n"
      ],
      "metadata": {
        "id": "otl4V60lElrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sample code to process the structured stream...\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Extract a sample JSON string to infer schema\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "\n",
        "# Create DataFrame representing the stream of input\n",
        "# lines from connection to logsender 7777\n",
        "try:\n",
        "  json_lines = spark.readStream.format(\"socket\") \\\n",
        "      .option(\"host\", \"localhost\") \\\n",
        "      .option(\"port\", 7777) \\\n",
        "      .load()\n",
        "\n",
        "  # Parse the JSON using the inferred schema\n",
        "  json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "    .select(\"json_data.*\")  # Expand the JSON fields into columns\n",
        "\n",
        "\n",
        "  query = json_lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime='15 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(60)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  query.stop()"
      ],
      "metadata": {
        "id": "5pzi95IkvgEZ",
        "outputId": "7abb4d9a-c865-47ce-92b9-4115e4b324fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+------+\n",
            "|date|energy|sensor|\n",
            "+----+------+------+\n",
            "+----+------+------+\n",
            "\n",
            "+-------------------+-------+------+\n",
            "|date               |energy |sensor|\n",
            "+-------------------+-------+------+\n",
            "|2024-10-01 00:04:21|2790.18|C     |\n",
            "|2024-10-01 00:04:27|5949.0 |D     |\n",
            "|2024-10-01 00:04:36|2162.37|J     |\n",
            "|2024-10-01 00:04:52|2682.69|I     |\n",
            "|2024-10-01 00:04:24|3993.9 |H     |\n",
            "|2024-10-01 00:04:33|3481.07|E     |\n",
            "|2024-10-01 00:04:43|1597.49|F     |\n",
            "+-------------------+-------+------+\n",
            "\n",
            "+-------------------+-------+------+\n",
            "|date               |energy |sensor|\n",
            "+-------------------+-------+------+\n",
            "|2024-10-01 00:14:30|2790.19|C     |\n",
            "|2024-10-01 00:14:42|3481.08|E     |\n",
            "|2024-10-01 00:14:48|1668.96|B     |\n",
            "|2024-10-01 00:14:54|1649.25|A     |\n",
            "|2024-10-01 00:15:00|2682.71|I     |\n",
            "|2024-10-01 00:24:41|3993.95|H     |\n",
            "|2024-10-01 00:24:51|3481.09|E     |\n",
            "|2024-10-01 00:25:03|1649.26|A     |\n",
            "|2024-10-01 00:25:09|2682.72|I     |\n",
            "|2024-10-01 00:14:36|5949.1 |D     |\n",
            "+-------------------+-------+------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-------------------+-------+------+\n",
            "|date               |energy |sensor|\n",
            "+-------------------+-------+------+\n",
            "|2024-10-01 00:34:47|2790.21|C     |\n",
            "|2024-10-01 00:34:53|5949.1 |D     |\n",
            "|2024-10-01 00:35:03|2162.51|J     |\n",
            "|2024-10-01 00:35:09|1597.51|F     |\n",
            "|2024-10-01 00:35:15|2081.02|G     |\n",
            "|2024-10-01 00:34:50|3993.96|H     |\n",
            "|2024-10-01 00:34:59|3481.1 |E     |\n",
            "|2024-10-01 00:35:06|1668.99|B     |\n",
            "|2024-10-01 00:35:12|1649.27|A     |\n",
            "|2024-10-01 00:35:18|2682.74|I     |\n",
            "+-------------------+-------+------+\n",
            "\n",
            "+-------------------+-------+------+\n",
            "|date               |energy |sensor|\n",
            "+-------------------+-------+------+\n",
            "|2024-10-01 00:44:56|2790.22|C     |\n",
            "|2024-10-01 00:45:02|5949.2 |D     |\n",
            "|2024-10-01 00:45:27|2682.75|I     |\n",
            "|2024-10-01 00:55:08|3993.98|H     |\n",
            "|2024-10-01 00:55:14|2147.11|K     |\n",
            "|2024-10-01 00:55:20|2162.52|J     |\n",
            "|2024-10-01 00:55:29|1649.5 |A     |\n",
            "|2024-10-01 00:44:59|3993.97|H     |\n",
            "|2024-10-01 00:45:20|1649.31|A     |\n",
            "|2024-10-01 00:55:05|2790.24|C     |\n",
            "+-------------------+-------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QUESTION 1\n",
        "\n",
        "#while making this project, we foud out that before running any code we had to restart the session and restart the stream source or the several queries on the different cell would get mixed up and the output would be confusing\n",
        "#throughout the different cells we ended up having to recreate several variables such as the sample_json because those variables weren´t being recognized in the cells where they weren´t created\n",
        "\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "#creates the spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# defines a sample JSON string representing a single data record\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "\n",
        "try:\n",
        "    # creates a data source to read the streamed data flow\n",
        "    json_lines = spark.readStream.format(\"socket\") \\\n",
        "        .option(\"host\", \"localhost\") \\\n",
        "        .option(\"port\", 7777) \\\n",
        "        .load()\n",
        "\n",
        "    # creates a new column \"json_data\" that convert the json string received in the data flow to the defined sample_json schema\n",
        "    json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "        .select(\"json_data.*\")\n",
        "\n",
        "    # creating the global variables to track the state of the sensors and the accumulated total. Without those global variables each batch would be treated independently and the running total wouldn´t be preserved and incremented\n",
        "    global sensor_states\n",
        "    sensor_states = {}\n",
        "    global running_total\n",
        "    running_total = 0.0\n",
        "\n",
        "    # creating a function that computes the running total for the group of sensors\n",
        "    def calculate_running_total(batch_df, batch_id):\n",
        "        global sensor_states, running_total\n",
        "\n",
        "        # .collect() converts the dataframe into a list of rows for processing and the for extracts the sensor identifier and correspondent energy value for each of those rows\n",
        "        for row in batch_df.collect():\n",
        "            sensor = row[\"sensor\"]\n",
        "            energy = row[\"energy\"]\n",
        "\n",
        "            #if the sensor isn´t yet in the sensor_states dictionary , it is added and the correspondent energy is used in the running total energy sum\n",
        "            if sensor not in sensor_states:\n",
        "                sensor_states[sensor] = energy\n",
        "                running_total += energy\n",
        "            else:\n",
        "                # if the sensor is already in the dictionary, the energy difference between that reading and the one before is added to the running total energy value\n",
        "                energy_difference = energy - sensor_states[sensor]\n",
        "                if energy_difference > 0:\n",
        "                    running_total += energy_difference\n",
        "                sensor_states[sensor] = energy\n",
        "\n",
        "        # print the running total energy for every mini-batch received\n",
        "        print(f\"Batch {batch_id} - Running Total Energy: {running_total}\")\n",
        "\n",
        "    # defining the query, append mode is used because we already have global variables that preserve the data between batches\n",
        "    query = json_lines \\\n",
        "        .writeStream \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .trigger(processingTime='25 seconds') \\\n",
        "        .foreachBatch(calculate_running_total) \\\n",
        "        .start()\n",
        "\n",
        "    query.awaitTermination(100)\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    if 'query' in locals():\n",
        "        query.stop()\n",
        "\n",
        "\n",
        "#this output was obtained with a trigger of 25 seconds to quickly evaluate wether or not the code is working well and an await termination of 100 seconds\n",
        "#depending on the trigger chosen, some early batches might not contain all the 11 sensors (for example,we explored the stream csv a little bit and found that the first reading of the K sensor only appears at 1 am) and the iterations of the running total between those batches might be somewhat significant"
      ],
      "metadata": {
        "id": "DgLxGIHsNLkf",
        "outputId": "898cdb86-ad21-4327-f6de-2d23ecd77940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 - Running Total Energy: 0.0\n",
            "Batch 1 - Running Total Energy: 28056.18\n",
            "Batch 2 - Running Total Energy: 28056.60999999999\n",
            "Batch 3 - Running Total Energy: 30204.389999999985\n",
            "Batch 4 - Running Total Energy: 30204.85999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QUESTION 2 - First Part\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, desc, row_number, sum, round\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "#In this first part of the second question, as an intermediate step, we reapllied the first project code to compute the running total energy for each day of february (available)\n",
        "\n",
        "#initiates a new spark session\n",
        "spark = SparkSession.builder.appName(\"ProcessFebruaryData\").getOrCreate()\n",
        "\n",
        "#uploads the february readings csv, specifying the different cell delimiter which is \";\"\n",
        "readings_february = spark.read.option(\"header\", \"false\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"delimiter\", \";\") \\\n",
        "    .csv(\"february-energy-readings.csv\")\n",
        "\n",
        "# renames the columns manually (the february csv didn´t appear to have the columns names)\n",
        "readings_february = readings_february.toDF(\"timestamp\", \"sensor\", \"energy\")\n",
        "\n",
        "# transform the 'timestamp' column from timestamo into DateType\n",
        "readings_february = readings_february.withColumn(\"date\", to_date(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "# select the last reading for each day of the month and for each sensor\n",
        "window_spec = Window.partitionBy(\"sensor\", \"date\").orderBy(desc(\"timestamp\"))\n",
        "daily_last_reading = readings_february.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
        "    .filter(col(\"row_number\") == 1) \\\n",
        "    .drop(\"row_number\")\n",
        "\n",
        "#sums the last readings of each sensor for each day (available)\n",
        "daily_running_total_february = daily_last_reading.groupBy(\"date\").agg(\n",
        "    round(sum(\"energy\"), 2).alias(\"running_total_energy_february\")\n",
        ").orderBy(\"date\")\n",
        "\n",
        "\n",
        "daily_running_total_february.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "rEzcj5toL_d_",
        "outputId": "e6e8f092-6f60-4897-9036-68b39233deea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------------------+\n",
            "|      date|running_total_energy_february|\n",
            "+----------+-----------------------------+\n",
            "|2024-02-01|                      13447.7|\n",
            "|2024-02-02|                      13517.9|\n",
            "|2024-02-09|                      14432.9|\n",
            "|2024-02-10|                      14547.4|\n",
            "|2024-02-11|                      14665.3|\n",
            "|2024-02-12|                      14776.1|\n",
            "|2024-02-13|                      14888.8|\n",
            "|2024-02-14|                      14982.2|\n",
            "|2024-02-15|                      15063.4|\n",
            "|2024-02-16|                      15111.2|\n",
            "|2024-02-18|                      15351.4|\n",
            "|2024-02-19|                      15431.1|\n",
            "|2024-02-20|                      15515.4|\n",
            "|2024-02-21|                      15598.4|\n",
            "|2024-02-22|                      15675.0|\n",
            "|2024-02-23|                      15759.8|\n",
            "|2024-02-24|                     15903.35|\n",
            "|2024-02-25|                     16003.15|\n",
            "|2024-02-26|                     16095.66|\n",
            "|2024-02-27|                     16188.81|\n",
            "+----------+-----------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QUESTION 2 - Second Part\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from datetime import datetime\n",
        "\n",
        "# creates the new spark session\n",
        "spark = SparkSession.builder.appName(\"StructuredWebLogExample\").getOrCreate()\n",
        "\n",
        "\n",
        "# once again,defines a sample JSON string representing a single data record (using the same variable in 2 different cells, for some reason, wasn´t really working )\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "# same global variables as before to preserve the running energy value between different batches\n",
        "sensor_states = {}\n",
        "running_total = 0.0\n",
        "\n",
        "# defining a function , similar to the one in the first question, that computes the running total energy for each dayn(group of sensors) and compares it to the running total energy of the same group of sensors in the month of february, the comparion being held day by day\n",
        "def calculate_daily_comparison(batch_df, batch_id):\n",
        "    global sensor_states, running_total, daily_running_total_february\n",
        "\n",
        "\n",
        "    for row in batch_df.collect():\n",
        "        sensor = row[\"sensor\"]\n",
        "        energy = row[\"energy\"]\n",
        "        timestamp = row[\"date\"]\n",
        "        #converts timestamp values into datetime values\n",
        "        current_date = datetime.strptime(timestamp.split(\" \")[0], \"%Y-%m-%d\").date()\n",
        "\n",
        "        # if given sensor not in the dictionary, add it there with the corresponding energy reading\n",
        "        if sensor not in sensor_states:\n",
        "            sensor_states[sensor] = energy\n",
        "            running_total += energy\n",
        "        else:\n",
        "            # exact same procedure as question 1\n",
        "            energy_difference = energy - sensor_states[sensor]\n",
        "            if energy_difference > 0:\n",
        "                running_total += energy_difference\n",
        "            sensor_states[sensor] = energy\n",
        "\n",
        "    # now the comparion with the february values\n",
        "    # adding a 'day' column (that extracts only the day of the 'date' column) to ensure that , for example, day 1 of october or november is compared with day 1 of february and so on\n",
        "    batch_df = batch_df.withColumn(\"day\", dayofmonth(\"date\"))\n",
        "    february_data = daily_running_total_february.withColumn(\"day\", dayofmonth(\"date\"))\n",
        "\n",
        "    # a new comparison df is created, internal join happens between the batch data and the february data, using the 'day' column, the current running total energy is then compared to the daily februaru running total and the percentage is computed\n",
        "    comparison = february_data.join(\n",
        "        batch_df.groupBy(\"day\").agg(lit(running_total).alias(\"running_total_stream\")),\n",
        "        \"day\",\n",
        "        \"inner\"\n",
        "    ).withColumn(\n",
        "        \"percentage\",\n",
        "        (col(\"running_total_stream\") / col(\"running_total_energy_february\")) * 100\n",
        "    )\n",
        "\n",
        "    # display the results , identifying the day used for comparison\n",
        "    comparison.select(\"day\", \"running_total_stream\", \"running_total_energy_february\", \"percentage\").show(truncate=False)\n",
        "\n",
        "try:\n",
        "    # read the socket stream\n",
        "    json_lines = spark.readStream.format(\"socket\") \\\n",
        "        .option(\"host\", \"localhost\") \\\n",
        "        .option(\"port\", 7777) \\\n",
        "        .load()\n",
        "\n",
        "\n",
        "    json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "        .select(\"json_data.*\")\n",
        "\n",
        "    # defining the query\n",
        "    query = json_lines \\\n",
        "        .writeStream \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .trigger(processingTime='25 seconds') \\\n",
        "        .foreachBatch(calculate_daily_comparison) \\\n",
        "        .start()\n",
        "\n",
        "    query.awaitTermination(100)\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    if 'query' in locals():\n",
        "        query.stop()\n",
        "\n",
        "# just like in question 1 , there´s a big difference between consecutive batches in the beggining, which corresponds to the late addition of sensors with a first reading that doesn´t appear initially\n"
      ],
      "metadata": {
        "id": "PlZx79WpTFUj",
        "outputId": "3f1d67f5-cf77-4e18-cfeb-ed629ac35671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+-----------------------------+----------+\n",
            "|day|running_total_stream|running_total_energy_february|percentage|\n",
            "+---+--------------------+-----------------------------+----------+\n",
            "+---+--------------------+-----------------------------+----------+\n",
            "\n",
            "+---+--------------------+-----------------------------+-----------------+\n",
            "|day|running_total_stream|running_total_energy_february|percentage       |\n",
            "+---+--------------------+-----------------------------+-----------------+\n",
            "|1  |22656.7             |13447.7                      |168.4801118406865|\n",
            "+---+--------------------+-----------------------------+-----------------+\n",
            "\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "|day|running_total_stream|running_total_energy_february|percentage        |\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "|1  |28056.40999999999   |13447.7                      |208.63352097384674|\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "|day|running_total_stream|running_total_energy_february|percentage        |\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "|1  |30203.939999999984  |13447.7                      |224.60301761639525|\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "|day|running_total_stream|running_total_energy_february|percentage        |\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "|1  |30204.849999999962  |13447.7                      |224.60978457282627|\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QUESTION 3\n",
        "\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# creating the new spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# same sample schema\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "global sensor_states\n",
        "sensor_states = {}\n",
        "global running_total\n",
        "running_total = 0.0\n",
        "\n",
        "# defining a function that, just like the others before, computes the running total energy in each batch and now compares the running total of each sensor with the sum of the 11 running total energy for the group of sensors\n",
        "def calculate_running_total_and_percentage(batch_df, batch_number):\n",
        "    global sensor_states, running_total\n",
        "\n",
        "    # same procedure as before\n",
        "    for row in batch_df.collect():\n",
        "        sensor = row[\"sensor\"]\n",
        "        energy = row[\"energy\"]\n",
        "\n",
        "        # same procedure as before, sensor not in sensor_states , it is added\n",
        "        if sensor not in sensor_states:\n",
        "            sensor_states[sensor] = energy\n",
        "            running_total += energy\n",
        "\n",
        "        else:\n",
        "            energy_difference = energy - sensor_states[sensor]\n",
        "\n",
        "            if energy_difference > 0:\n",
        "                running_total += energy_difference\n",
        "            sensor_states[sensor] = energy\n",
        "\n",
        "    # compute the percentage for each sensor relative to the total running energy value  (for each batch) and stores those values in a list called sensor_percentages\n",
        "    sensor_percentages = []\n",
        "    #the for cycle iterates over the dictionary sensor_states that, for each batch, contains the current state of each sensor aka the current energy value for each one\n",
        "    for sensor, energy in sensor_states.items():\n",
        "        percentage = (energy / running_total) * 100 if running_total > 0 else 0\n",
        "        sensor_percentages.append((sensor, percentage))\n",
        "\n",
        "    # sorting the values in the list by percentage by acessing the second element of the tuple, x[1], which is the percentage value, and creating a new list with the same name with the values correctly sorted\n",
        "    sorted_percentages = sorted(sensor_percentages, key=lambda x: -x[1])\n",
        "\n",
        "    # displaying the results sorted out\n",
        "    print(f\"Batch {batch_number} - Running Total Energy: {running_total}\")\n",
        "    print(\"Individual Sensor Percentages:\")\n",
        "    for sensor, percentage in sorted_percentages:\n",
        "        print(f\"  Sensor {sensor}: {percentage:.2f}%\")\n",
        "\n",
        "try:\n",
        "\n",
        "    json_lines = spark.readStream.format(\"socket\") \\\n",
        "        .option(\"host\", \"localhost\") \\\n",
        "        .option(\"port\", 7777) \\\n",
        "        .load()\n",
        "\n",
        "    json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "        .select(\"json_data.*\")\n",
        "\n",
        "#defining the query\n",
        "    query = json_lines \\\n",
        "        .writeStream \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .trigger(processingTime='25 seconds') \\\n",
        "        .foreachBatch(calculate_running_total_and_percentage) \\\n",
        "        .start()\n",
        "\n",
        "    query.awaitTermination(200)\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    if 'query' in locals():\n",
        "        query.stop()\n",
        "\n",
        "\n",
        "#with a trigger of 25 seconds, the iterations in the percentages happen very rarely, due to the fact that the running energy for each sensor increases very slowly\n"
      ],
      "metadata": {
        "id": "mjexFK_GraHo",
        "outputId": "35ef5099-ae0b-4803-aa5a-0efa53daf357",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 - Running Total Energy: 0.0\n",
            "Individual Sensor Percentages:\n",
            "Batch 1 - Running Total Energy: 22656.7\n",
            "Individual Sensor Percentages:\n",
            "  Sensor D: 26.26%\n",
            "  Sensor H: 17.63%\n",
            "  Sensor E: 15.36%\n",
            "  Sensor C: 12.32%\n",
            "  Sensor I: 11.84%\n",
            "  Sensor J: 9.54%\n",
            "  Sensor F: 7.05%\n",
            "Batch 2 - Running Total Energy: 28056.309999999987\n",
            "Individual Sensor Percentages:\n",
            "  Sensor D: 21.20%\n",
            "  Sensor H: 14.24%\n",
            "  Sensor E: 12.41%\n",
            "  Sensor C: 9.94%\n",
            "  Sensor I: 9.56%\n",
            "  Sensor J: 7.71%\n",
            "  Sensor G: 7.42%\n",
            "  Sensor B: 5.95%\n",
            "  Sensor A: 5.88%\n",
            "  Sensor F: 5.69%\n",
            "Batch 3 - Running Total Energy: 30203.989999999972\n",
            "Individual Sensor Percentages:\n",
            "  Sensor D: 19.70%\n",
            "  Sensor H: 13.22%\n",
            "  Sensor E: 11.53%\n",
            "  Sensor C: 9.24%\n",
            "  Sensor I: 8.88%\n",
            "  Sensor J: 7.16%\n",
            "  Sensor K: 7.11%\n",
            "  Sensor G: 6.89%\n",
            "  Sensor B: 5.53%\n",
            "  Sensor A: 5.46%\n",
            "  Sensor F: 5.29%\n",
            "Batch 4 - Running Total Energy: 30204.77999999997\n",
            "Individual Sensor Percentages:\n",
            "  Sensor D: 19.70%\n",
            "  Sensor H: 13.22%\n",
            "  Sensor E: 11.53%\n",
            "  Sensor C: 9.24%\n",
            "  Sensor I: 8.88%\n",
            "  Sensor J: 7.16%\n",
            "  Sensor K: 7.11%\n",
            "  Sensor G: 6.89%\n",
            "  Sensor B: 5.53%\n",
            "  Sensor A: 5.46%\n",
            "  Sensor F: 5.29%\n",
            "Batch 5 - Running Total Energy: 30205.04999999996\n",
            "Individual Sensor Percentages:\n",
            "  Sensor D: 19.70%\n",
            "  Sensor H: 13.22%\n",
            "  Sensor E: 11.53%\n",
            "  Sensor C: 9.24%\n",
            "  Sensor I: 8.88%\n",
            "  Sensor J: 7.16%\n",
            "  Sensor K: 7.11%\n",
            "  Sensor G: 6.89%\n",
            "  Sensor B: 5.53%\n",
            "  Sensor A: 5.46%\n",
            "  Sensor F: 5.29%\n",
            "Batch 6 - Running Total Energy: 30205.249999999953\n",
            "Individual Sensor Percentages:\n",
            "  Sensor D: 19.70%\n",
            "  Sensor H: 13.22%\n",
            "  Sensor E: 11.53%\n",
            "  Sensor C: 9.24%\n",
            "  Sensor I: 8.88%\n",
            "  Sensor J: 7.16%\n",
            "  Sensor K: 7.11%\n",
            "  Sensor G: 6.89%\n",
            "  Sensor B: 5.53%\n",
            "  Sensor A: 5.46%\n",
            "  Sensor F: 5.29%\n",
            "Batch 7 - Running Total Energy: 30205.46999999994\n",
            "Individual Sensor Percentages:\n",
            "  Sensor D: 19.70%\n",
            "  Sensor H: 13.22%\n",
            "  Sensor E: 11.53%\n",
            "  Sensor C: 9.24%\n",
            "  Sensor I: 8.88%\n",
            "  Sensor J: 7.16%\n",
            "  Sensor K: 7.11%\n",
            "  Sensor G: 6.89%\n",
            "  Sensor B: 5.53%\n",
            "  Sensor A: 5.46%\n",
            "  Sensor F: 5.29%\n",
            "Batch 8 - Running Total Energy: 30205.75999999993\n",
            "Individual Sensor Percentages:\n",
            "  Sensor D: 19.70%\n",
            "  Sensor H: 13.22%\n",
            "  Sensor E: 11.53%\n",
            "  Sensor C: 9.24%\n",
            "  Sensor I: 8.88%\n",
            "  Sensor J: 7.16%\n",
            "  Sensor K: 7.11%\n",
            "  Sensor G: 6.89%\n",
            "  Sensor B: 5.53%\n",
            "  Sensor A: 5.46%\n",
            "  Sensor F: 5.29%\n"
          ]
        }
      ]
    }
  ]
}