{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diogocristovao/SPBD/blob/main/docs/labs/projs/spbd2425_tp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Sistemas para Processamento de Big Data\n",
        "## TP2 - Energy Meter Live Monitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sensor data corresponds to regular readings from 11 residential energy meters. The data covers the month of February 2024.\n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "timestamp | sensor_id | energy\n",
        "----------|-------------|-----------\n",
        "timestamp | string  | float\n",
        "\n",
        "Each energy value (KWh) corresponds to the accumulated value of the meter at the time of measurement. As such,\n",
        "each meter is expected to produce a monotonically increasing series of pairs of timestamp and energy consummed up to that moment.\n",
        "\n",
        "The meters do not start at zero or at the same value.\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "For all the sensors combined:\n",
        "\n",
        "1. For the current month and current day, compute the running total energy consumed so far. The values should be updated every 5 minutes.\n",
        "\n",
        "2. For the current month and current day, compute the running total energy consumed so far, **as a percentage**, **compared to the same periods in February 2024**. The values should be updated every 5 minutes.\n",
        "\n",
        "For each sensor, separately:\n",
        "\n",
        "3. For the current month and current day, compute the running total energy consumed so far, as a percentage, **comparing the value of each individual sensor, relative to the same results for all the sensors together (as in #1)**. The values should be updated every 5 minutes. (Sorted in descending order by value and sensor.)\n",
        "\n",
        "**Note:** For simplicity, it is fine to assume the first reading of each day can be used to start counting how much energy has been consumed so far. There is no need to interpolate/estimate the value of the meters at midnight.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "Solve each question using Structured Spark Streaming."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Grading Criteria\n",
        "\n",
        "+ Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "\n",
        "December 6.\n",
        "\n",
        "Penalty of 0.25 grade points per day late.\n",
        "\n",
        "Penalty accumulates until the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark --quiet"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Archived February Energy Readings\n",
        "!wget -q -O /tmp/readings.csv https://raw.githubusercontent.com/smduarte/spbd-2425/refs/heads/main/docs/labs/projs/energy-readings.csv\n",
        "!grep \"2024-02\" /tmp/readings.csv > february-energy-readings.csv\n",
        "!head -2 february-energy-readings.csv\n"
      ],
      "metadata": {
        "id": "w38DVs9wBhhu",
        "outputId": "a4586d9a-6d7b-4a44-87ca-aa5477757436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-01 00:00:00;D;2615.0\n",
            "2024-02-01 00:00:18;C;1098.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start the Structured Source\n",
        "!wget -q -O - https://github.com/smduarte/spbd-2425/raw/main/scripts/json_energy_sender.tgz  | tar xfz - 2> /dev/null\n",
        "\n",
        "!nohup python json_energy_sender/server.py --filename json_energy_sender/energy-readings.csv --speedup 60 > /dev/null 2> /dev/null &"
      ],
      "metadata": {
        "id": "n4fbaIE0vedz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note: --speedup 60, means the stream is played 60x faster than realtime. Therefore, 1 second in real time corresponds to 1 minute of stream data.\n"
      ],
      "metadata": {
        "id": "otl4V60lElrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sample code to process the structured stream...\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Extract a sample JSON string to infer schema\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "\n",
        "# Create DataFrame representing the stream of input\n",
        "# lines from connection to logsender 7777\n",
        "try:\n",
        "  json_lines = spark.readStream.format(\"socket\") \\\n",
        "      .option(\"host\", \"localhost\") \\\n",
        "      .option(\"port\", 7777) \\\n",
        "      .load()\n",
        "\n",
        "  # Parse the JSON using the inferred schema\n",
        "  json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "    .select(\"json_data.*\")  # Expand the JSON fields into columns\n",
        "\n",
        "\n",
        "  query = json_lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime='1 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(60)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  query.stop()"
      ],
      "metadata": {
        "id": "5pzi95IkvgEZ",
        "outputId": "69ebae15-b74a-4a07-f62a-78b6a6e9b996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+------+\n",
            "|date|energy|sensor|\n",
            "+----+------+------+\n",
            "+----+------+------+\n",
            "\n",
            "+-------------------+-------+------+\n",
            "|date               |energy |sensor|\n",
            "+-------------------+-------+------+\n",
            "|2024-10-01 00:04:21|2790.18|C     |\n",
            "|2024-10-01 00:04:27|5949.0 |D     |\n",
            "|2024-10-01 00:04:36|2162.37|J     |\n",
            "|2024-10-01 00:04:52|2682.69|I     |\n",
            "|2024-10-01 00:04:24|3993.9 |H     |\n",
            "|2024-10-01 00:04:33|3481.07|E     |\n",
            "|2024-10-01 00:04:43|1597.49|F     |\n",
            "+-------------------+-------+------+\n",
            "\n",
            "+-------------------+-------+------+\n",
            "|date               |energy |sensor|\n",
            "+-------------------+-------+------+\n",
            "|2024-10-01 00:14:30|2790.19|C     |\n",
            "|2024-10-01 00:14:42|3481.08|E     |\n",
            "|2024-10-01 00:14:48|1668.96|B     |\n",
            "|2024-10-01 00:14:54|1649.25|A     |\n",
            "|2024-10-01 00:14:36|5949.1 |D     |\n",
            "|2024-10-01 00:14:45|2162.5 |J     |\n",
            "|2024-10-01 00:14:51|1597.5 |F     |\n",
            "+-------------------+-------+------+\n",
            "\n",
            "+-------------------+-------+------+\n",
            "|date               |energy |sensor|\n",
            "+-------------------+-------+------+\n",
            "|2024-10-01 00:14:57|2080.99|G     |\n",
            "|2024-10-01 00:15:00|2682.71|I     |\n",
            "+-------------------+-------+------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=41>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o13.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while calling o47.awaitTermination\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QUESTION 1\n",
        "\n",
        "#while making this project, we foud out that before running any code we had to restart the session and restart the stream source or the several queries on the different cell would get mixed up and the output would be confusing\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "#creates the spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# defines a sample JSON string representing a single data record\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "\n",
        "try:\n",
        "    # creates a data source to read the streamed data flow\n",
        "    json_lines = spark.readStream.format(\"socket\") \\\n",
        "        .option(\"host\", \"localhost\") \\\n",
        "        .option(\"port\", 7777) \\\n",
        "        .load()\n",
        "\n",
        "    # creates a new column \"json_data\" that convert the json string received in the data flow to the defined sample_json schema\n",
        "    json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "        .select(\"json_data.*\")\n",
        "\n",
        "    # creating the global variables to track the state of the sensors and the accumulated total. Without those global variables each batch would be treated independently and the running total wouldn´t be preserved and incremented\n",
        "    global sensor_states\n",
        "    sensor_states = {}\n",
        "    global running_total\n",
        "    running_total = 0.0\n",
        "\n",
        "    # creating a function that computes the running total for the group of sensors\n",
        "    def calculate_running_total(batch_df, batch_id):\n",
        "        global sensor_states, running_total\n",
        "\n",
        "        # .collect() converts the dataframe into a list of rows for processing and the for extracts the sensor identifier and correspondent energy value for each of those rows\n",
        "        for row in batch_df.collect():\n",
        "            sensor = row[\"sensor\"]\n",
        "            energy = row[\"energy\"]\n",
        "\n",
        "            #if the sensor isn´t yet in the sensor_states dictionary , it is added and the correspondent energy is used in the running total energy sum\n",
        "            if sensor not in sensor_states:\n",
        "                sensor_states[sensor] = energy\n",
        "                running_total += energy\n",
        "            else:\n",
        "                # if the sensor is already in the dictionary, the energy difference between that reading and the one before is added to the running total energy value\n",
        "                energy_difference = energy - sensor_states[sensor]\n",
        "                if energy_difference > 0:\n",
        "                    running_total += energy_difference\n",
        "                sensor_states[sensor] = energy\n",
        "\n",
        "        # print the running total energy for every mini-batch received\n",
        "        print(f\"Batch {batch_id} - Running Total Energy: {running_total}\")\n",
        "\n",
        "    # defining the query, append mode is used because we already have global variables that preserve the data between batches\n",
        "    query = json_lines \\\n",
        "        .writeStream \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .trigger(processingTime='25 seconds') \\\n",
        "        .foreachBatch(calculate_running_total) \\\n",
        "        .start()\n",
        "\n",
        "    query.awaitTermination(100)\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    if 'query' in locals():\n",
        "        query.stop()\n",
        "\n",
        "\n",
        "#this output was obtained with a trigger of 25 seconds and an await termination of 100 seconds\n",
        "#depending on the trigger chosen, some early batches might not contain all the 11 sensors (for example, the first reading of the K sensor only appears at 1 am) and the iterations of the running total between those batches might be somewhat significant"
      ],
      "metadata": {
        "id": "DgLxGIHsNLkf",
        "outputId": "079f9d94-a855-42e8-e6f6-c08d43c46cc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 - Running Total Energy: 0.0\n",
            "Batch 1 - Running Total Energy: 28056.309999999998\n",
            "Batch 2 - Running Total Energy: 28056.619999999984\n",
            "Batch 3 - Running Total Energy: 30204.40999999998\n",
            "Batch 4 - Running Total Energy: 30204.94999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QUESTION 2 - First Part\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, desc, row_number, sum, round\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "#In this first part of the second question, as an intermediate step, we reapllied the first project code to compute the running total energy for each day of february (available)\n",
        "\n",
        "#initiates a new spark session\n",
        "spark = SparkSession.builder.appName(\"ProcessFebruaryData\").getOrCreate()\n",
        "\n",
        "#uploads the february readings csv, specifying the different cell delimiter which is \";\"\n",
        "readings_february = spark.read.option(\"header\", \"false\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"delimiter\", \";\") \\\n",
        "    .csv(\"february-energy-readings.csv\")\n",
        "\n",
        "# renames the columns manually (the february csv didn´t appear to have the columns names)\n",
        "readings_february = readings_february.toDF(\"timestamp\", \"sensor\", \"energy\")\n",
        "\n",
        "# transform the 'timestamp' column from timestamo into DateType\n",
        "readings_february = readings_february.withColumn(\"date\", to_date(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "# select the last reading for each day of the month and for each sensor\n",
        "window_spec = Window.partitionBy(\"sensor\", \"date\").orderBy(desc(\"timestamp\"))\n",
        "daily_last_reading = readings_february.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
        "    .filter(col(\"row_number\") == 1) \\\n",
        "    .drop(\"row_number\")\n",
        "\n",
        "# Calcular o total acumulado diário para os sensores combinados\n",
        "daily_running_total_february = daily_last_reading.groupBy(\"date\").agg(\n",
        "    round(sum(\"energy\"), 2).alias(\"running_total_energy_february\")\n",
        ").orderBy(\"date\")\n",
        "\n",
        "# Declarar a variável global\n",
        "\n",
        "#february_totals = {row[\"date\"]: row[\"running_total_energy_february\"] for row in daily_running_total_february.collect()}\n",
        "\n",
        "daily_running_total_february.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "rEzcj5toL_d_",
        "outputId": "77c8cdc6-81b9-453c-af0a-b385ccc66cf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------------------+\n",
            "|      date|running_total_energy_february|\n",
            "+----------+-----------------------------+\n",
            "|2024-02-01|                      13447.7|\n",
            "|2024-02-02|                      13517.9|\n",
            "|2024-02-09|                      14432.9|\n",
            "|2024-02-10|                      14547.4|\n",
            "|2024-02-11|                      14665.3|\n",
            "|2024-02-12|                      14776.1|\n",
            "|2024-02-13|                      14888.8|\n",
            "|2024-02-14|                      14982.2|\n",
            "|2024-02-15|                      15063.4|\n",
            "|2024-02-16|                      15111.2|\n",
            "|2024-02-18|                      15351.4|\n",
            "|2024-02-19|                      15431.1|\n",
            "|2024-02-20|                      15515.4|\n",
            "|2024-02-21|                      15598.4|\n",
            "|2024-02-22|                      15675.0|\n",
            "|2024-02-23|                      15759.8|\n",
            "|2024-02-24|                     15903.35|\n",
            "|2024-02-25|                     16003.15|\n",
            "|2024-02-26|                     16095.66|\n",
            "|2024-02-27|                     16188.81|\n",
            "+----------+-----------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QUESTÃO 2 - 2ºParte\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from datetime import datetime\n",
        "\n",
        "# JSON de exemplo para inferir o esquema\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "# Variáveis globais para rastrear o estado dos sensores e o total acumulado\n",
        "sensor_states = {}\n",
        "running_total = 0.0\n",
        "\n",
        "# Função para calcular e comparar o total diário\n",
        "def calculate_daily_comparison(batch_df, epoch_id):\n",
        "    global sensor_states, running_total, daily_running_total_february\n",
        "\n",
        "    # Atualizar o estado e calcular a soma incremental para cada sensor\n",
        "    for row in batch_df.collect():\n",
        "        sensor = row[\"sensor\"]\n",
        "        energy = row[\"energy\"]\n",
        "        timestamp = row[\"date\"]\n",
        "        current_date = datetime.strptime(timestamp.split(\" \")[0], \"%Y-%m-%d\").date()\n",
        "\n",
        "        # Atualizar o estado de cada sensor\n",
        "        if sensor not in sensor_states:\n",
        "            sensor_states[sensor] = energy\n",
        "            running_total += energy\n",
        "        else:\n",
        "            # Calcular a diferença de energia e atualizar o total acumulado\n",
        "            energy_difference = energy - sensor_states[sensor]\n",
        "            if energy_difference > 0:  # Garantir que só valores crescentes sejam somados\n",
        "                running_total += energy_difference\n",
        "            sensor_states[sensor] = energy\n",
        "\n",
        "    # Ajuste para comparação com fevereiro\n",
        "    # Transformar a data para apenas o dia (desconsiderando o mês e ano)\n",
        "    batch_df = batch_df.withColumn(\"day\", dayofmonth(\"date\"))\n",
        "    february_data = daily_running_total_february.withColumn(\"day\", dayofmonth(\"date\"))\n",
        "\n",
        "    # Fazer o join com o DataFrame de fevereiro\n",
        "    comparison = february_data.join(\n",
        "        batch_df.groupBy(\"day\").agg(lit(running_total).alias(\"running_total_stream\")),\n",
        "        \"day\",\n",
        "        \"inner\"\n",
        "    ).withColumn(\n",
        "        \"percentage\",\n",
        "        (col(\"running_total_stream\") / col(\"running_total_energy_february\")) * 100\n",
        "    )\n",
        "\n",
        "    # Exibir os resultados\n",
        "    comparison.select(\"day\", \"running_total_stream\", \"running_total_energy_february\", \"percentage\").show(truncate=False)\n",
        "\n",
        "# Criar a sessão do Spark\n",
        "spark = SparkSession.builder.appName(\"StructuredWebLogExample\").getOrCreate()\n",
        "\n",
        "# Criar DataFrame representando o fluxo de entrada\n",
        "try:\n",
        "    # Ler o fluxo do socket\n",
        "    json_lines = spark.readStream.format(\"socket\") \\\n",
        "        .option(\"host\", \"localhost\") \\\n",
        "        .option(\"port\", 7777) \\\n",
        "        .load()\n",
        "\n",
        "    # Analisar o JSON e expandir os campos\n",
        "    json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "        .select(\"json_data.*\")\n",
        "\n",
        "    # Definir a consulta para processar o fluxo\n",
        "    query = json_lines \\\n",
        "        .writeStream \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .trigger(processingTime='25 seconds') \\\n",
        "        .foreachBatch(calculate_daily_comparison) \\\n",
        "        .start()\n",
        "\n",
        "    query.awaitTermination(100)\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    if 'query' in locals():\n",
        "        query.stop()\n"
      ],
      "metadata": {
        "id": "PlZx79WpTFUj",
        "outputId": "e27d23dc-afd6-4a06-9703-dea982831882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+-----------------------------+----------+\n",
            "|day|running_total_stream|running_total_energy_february|percentage|\n",
            "+---+--------------------+-----------------------------+----------+\n",
            "+---+--------------------+-----------------------------+----------+\n",
            "\n",
            "+---+--------------------+-----------------------------+-----------------+\n",
            "|day|running_total_stream|running_total_energy_february|percentage       |\n",
            "+---+--------------------+-----------------------------+-----------------+\n",
            "|1  |28056.18            |13447.7                      |208.6318106441994|\n",
            "+---+--------------------+-----------------------------+-----------------+\n",
            "\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "|day|running_total_stream|running_total_energy_february|percentage        |\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "|1  |28056.41999999999   |13447.7                      |208.63359533600533|\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "|day|running_total_stream|running_total_energy_february|percentage        |\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "|1  |30204.539999999986  |13447.7                      |224.60747934591035|\n",
            "+---+--------------------+-----------------------------+------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d7f57fdd2e95>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    217\u001b[0m                     \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 )\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QYESTÃO 3\n",
        "\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Criar a sessão do Spark\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# JSON de exemplo para inferir o esquema\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "# Criar DataFrame representando o fluxo de entrada\n",
        "try:\n",
        "    # Ler o fluxo do socket\n",
        "    json_lines = spark.readStream.format(\"socket\") \\\n",
        "        .option(\"host\", \"localhost\") \\\n",
        "        .option(\"port\", 7777) \\\n",
        "        .load()\n",
        "\n",
        "    # Analisar o JSON e expandir os campos\n",
        "    json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "        .select(\"json_data.*\")  # Expandir os campos do JSON para colunas\n",
        "\n",
        "    # Variáveis globais para rastrear o estado dos sensores e o total acumulado\n",
        "    global sensor_states\n",
        "    sensor_states = {}\n",
        "    global running_total\n",
        "    running_total = 0.0\n",
        "\n",
        "    # Função para calcular e exibir a soma acumulada e porcentagem por sensor\n",
        "    def calculate_running_total_and_percentage(batch_df, epoch_id):\n",
        "        global sensor_states, running_total\n",
        "\n",
        "        # Atualizar o estado e calcular a soma incremental\n",
        "        for row in batch_df.collect():\n",
        "            sensor = row[\"sensor\"]\n",
        "            energy = row[\"energy\"]\n",
        "\n",
        "            # Se o sensor não estiver no estado, inicialize com a primeira leitura\n",
        "            if sensor not in sensor_states:\n",
        "                sensor_states[sensor] = energy\n",
        "                running_total += energy\n",
        "            else:\n",
        "                # Calcular a diferença entre a leitura atual e a última conhecida\n",
        "                energy_difference = energy - sensor_states[sensor]\n",
        "\n",
        "                # Atualizar o total acumulado e o estado do sensor\n",
        "                if energy_difference > 0:  # Garantir que só valores crescentes sejam somados\n",
        "                    running_total += energy_difference\n",
        "                sensor_states[sensor] = energy\n",
        "\n",
        "        # Calcular e exibir os percentuais por sensor\n",
        "        sensor_percentages = []\n",
        "        for sensor, energy in sensor_states.items():\n",
        "            percentage = (energy / running_total) * 100 if running_total > 0 else 0\n",
        "            sensor_percentages.append((sensor, percentage))\n",
        "\n",
        "        # Ordenar em ordem decrescente de porcentagem e por nome do sensor\n",
        "        sorted_percentages = sorted(sensor_percentages, key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "        # Exibir os resultados\n",
        "        print(f\"Batch {epoch_id} - Running Total Energy: {running_total}\")\n",
        "        print(\"Sensor Percentages (sorted):\")\n",
        "        for sensor, percentage in sorted_percentages:\n",
        "            print(f\"  Sensor {sensor}: {percentage:.2f}%\")\n",
        "\n",
        "    # Definir a consulta para processar o fluxo\n",
        "    query = json_lines \\\n",
        "        .writeStream \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .trigger(processingTime='30 seconds') \\\n",
        "        .foreachBatch(calculate_running_total_and_percentage) \\\n",
        "        .start()\n",
        "\n",
        "    query.awaitTermination(500)\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    if 'query' in locals():\n",
        "        query.stop()\n"
      ],
      "metadata": {
        "id": "mjexFK_GraHo",
        "outputId": "a179efe1-0626-4a2e-821f-e4de5d5b47be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 - Running Total Energy: 0.0\n",
            "Sensor Percentages (sorted):\n",
            "Batch 1 - Running Total Energy: 28056.18\n",
            "Sensor Percentages (sorted):\n",
            "  Sensor D: 21.20%\n",
            "  Sensor H: 14.24%\n",
            "  Sensor E: 12.41%\n",
            "  Sensor C: 9.95%\n",
            "  Sensor I: 9.56%\n",
            "  Sensor J: 7.71%\n",
            "  Sensor G: 7.42%\n",
            "  Sensor B: 5.95%\n",
            "  Sensor A: 5.88%\n",
            "  Sensor F: 5.69%\n",
            "Batch 2 - Running Total Energy: 28056.60999999999\n",
            "Sensor Percentages (sorted):\n",
            "  Sensor D: 21.20%\n",
            "  Sensor H: 14.24%\n",
            "  Sensor E: 12.41%\n",
            "  Sensor C: 9.94%\n",
            "  Sensor I: 9.56%\n",
            "  Sensor J: 7.71%\n",
            "  Sensor G: 7.42%\n",
            "  Sensor B: 5.95%\n",
            "  Sensor A: 5.88%\n",
            "  Sensor F: 5.69%\n",
            "Batch 3 - Running Total Energy: 30204.559999999983\n",
            "Sensor Percentages (sorted):\n",
            "  Sensor D: 19.70%\n",
            "  Sensor H: 13.22%\n",
            "  Sensor E: 11.53%\n",
            "  Sensor C: 9.24%\n",
            "  Sensor I: 8.88%\n",
            "  Sensor J: 7.16%\n",
            "  Sensor K: 7.11%\n",
            "  Sensor G: 6.89%\n",
            "  Sensor B: 5.53%\n",
            "  Sensor A: 5.46%\n",
            "  Sensor F: 5.29%\n",
            "Batch 4 - Running Total Energy: 30205.079999999973\n",
            "Sensor Percentages (sorted):\n",
            "  Sensor D: 19.70%\n",
            "  Sensor H: 13.22%\n",
            "  Sensor E: 11.53%\n",
            "  Sensor C: 9.24%\n",
            "  Sensor I: 8.88%\n",
            "  Sensor J: 7.16%\n",
            "  Sensor K: 7.11%\n",
            "  Sensor G: 6.89%\n",
            "  Sensor B: 5.53%\n",
            "  Sensor A: 5.46%\n",
            "  Sensor F: 5.29%\n",
            "Batch 5 - Running Total Energy: 30205.369999999966\n",
            "Sensor Percentages (sorted):\n",
            "  Sensor D: 19.70%\n",
            "  Sensor H: 13.22%\n",
            "  Sensor E: 11.53%\n",
            "  Sensor C: 9.24%\n",
            "  Sensor I: 8.88%\n",
            "  Sensor J: 7.16%\n",
            "  Sensor K: 7.11%\n",
            "  Sensor G: 6.89%\n",
            "  Sensor B: 5.53%\n",
            "  Sensor A: 5.46%\n",
            "  Sensor F: 5.29%\n",
            "Batch 6 - Running Total Energy: 30205.709999999963\n",
            "Sensor Percentages (sorted):\n",
            "  Sensor D: 19.70%\n",
            "  Sensor H: 13.22%\n",
            "  Sensor E: 11.53%\n",
            "  Sensor C: 9.24%\n",
            "  Sensor I: 8.88%\n",
            "  Sensor J: 7.16%\n",
            "  Sensor K: 7.11%\n",
            "  Sensor G: 6.89%\n",
            "  Sensor B: 5.53%\n",
            "  Sensor A: 5.46%\n",
            "  Sensor F: 5.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=41>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o13.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while calling o47.awaitTermination\n"
          ]
        }
      ]
    }
  ]
}