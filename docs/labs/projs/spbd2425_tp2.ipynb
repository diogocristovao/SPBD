{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diogocristovao/SPBD/blob/main/docs/labs/projs/spbd2425_tp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Sistemas para Processamento de Big Data\n",
        "## TP2 - Energy Meter Live Monitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sensor data corresponds to regular readings from 11 residential energy meters. The data covers the month of February 2024.\n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "timestamp | sensor_id | energy\n",
        "----------|-------------|-----------\n",
        "timestamp | string  | float\n",
        "\n",
        "Each energy value (KWh) corresponds to the accumulated value of the meter at the time of measurement. As such,\n",
        "each meter is expected to produce a monotonically increasing series of pairs of timestamp and energy consummed up to that moment.\n",
        "\n",
        "The meters do not start at zero or at the same value.\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "For all the sensors combined:\n",
        "\n",
        "1. For the current month and current day, compute the running total energy consumed so far. The values should be updated every 5 minutes.\n",
        "\n",
        "2. For the current month and current day, compute the running total energy consumed so far, **as a percentage**, **compared to the same periods in February 2024**. The values should be updated every 5 minutes.\n",
        "\n",
        "For each sensor, separately:\n",
        "\n",
        "3. For the current month and current day, compute the running total energy consumed so far, as a percentage, **comparing the value of each individual sensor, relative to the same results for all the sensors together (as in #1)**. The values should be updated every 5 minutes. (Sorted in descending order by value and sensor.)\n",
        "\n",
        "**Note:** For simplicity, it is fine to assume the first reading of each day can be used to start counting how much energy has been consumed so far. There is no need to interpolate/estimate the value of the meters at midnight.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "Solve each question using Structured Spark Streaming."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Grading Criteria\n",
        "\n",
        "+ Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "\n",
        "December 6.\n",
        "\n",
        "Penalty of 0.25 grade points per day late.\n",
        "\n",
        "Penalty accumulates until the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark --quiet"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Archived February Energy Readings\n",
        "!wget -q -O /tmp/readings.csv https://raw.githubusercontent.com/smduarte/spbd-2425/refs/heads/main/docs/labs/projs/energy-readings.csv\n",
        "!grep \"2024-02\" /tmp/readings.csv > february-energy-readings.csv\n",
        "!head -2 february-energy-readings.csv\n"
      ],
      "metadata": {
        "id": "w38DVs9wBhhu",
        "outputId": "2b314b17-2468-46f9-b3e1-a6f84945a96b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-01 00:00:00;D;2615.0\n",
            "2024-02-01 00:00:18;C;1098.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start the Structured Source\n",
        "!wget -q -O - https://github.com/smduarte/spbd-2425/raw/main/scripts/json_energy_sender.tgz  | tar xfz - 2> /dev/null\n",
        "\n",
        "!nohup python json_energy_sender/server.py --filename json_energy_sender/energy-readings.csv --speedup 60 > /dev/null 2> /dev/null &"
      ],
      "metadata": {
        "id": "n4fbaIE0vedz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note: --speedup 60, means the stream is played 60x faster than realtime. Therefore, 1 second in real time corresponds to 1 minute of stream data.\n"
      ],
      "metadata": {
        "id": "otl4V60lElrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sample code to process the structured stream...\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# Extract a sample JSON string to infer schema\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "\n",
        "# Create DataFrame representing the stream of input\n",
        "# lines from connection to logsender 7777\n",
        "try:\n",
        "  json_lines = spark.readStream.format(\"socket\") \\\n",
        "      .option(\"host\", \"localhost\") \\\n",
        "      .option(\"port\", 7777) \\\n",
        "      .load()\n",
        "\n",
        "  # Parse the JSON using the inferred schema\n",
        "  json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "    .select(\"json_data.*\")  # Expand the JSON fields into columns\n",
        "\n",
        "\n",
        "  query = json_lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime='1 seconds') \\\n",
        "    .foreachBatch(lambda df, epoch: df.show(10, False)) \\\n",
        "    .start()\n",
        "\n",
        "  query.awaitTermination(60)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "  query.stop()"
      ],
      "metadata": {
        "id": "5pzi95IkvgEZ",
        "outputId": "69ebae15-b74a-4a07-f62a-78b6a6e9b996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+------+\n",
            "|date|energy|sensor|\n",
            "+----+------+------+\n",
            "+----+------+------+\n",
            "\n",
            "+-------------------+-------+------+\n",
            "|date               |energy |sensor|\n",
            "+-------------------+-------+------+\n",
            "|2024-10-01 00:04:21|2790.18|C     |\n",
            "|2024-10-01 00:04:27|5949.0 |D     |\n",
            "|2024-10-01 00:04:36|2162.37|J     |\n",
            "|2024-10-01 00:04:52|2682.69|I     |\n",
            "|2024-10-01 00:04:24|3993.9 |H     |\n",
            "|2024-10-01 00:04:33|3481.07|E     |\n",
            "|2024-10-01 00:04:43|1597.49|F     |\n",
            "+-------------------+-------+------+\n",
            "\n",
            "+-------------------+-------+------+\n",
            "|date               |energy |sensor|\n",
            "+-------------------+-------+------+\n",
            "|2024-10-01 00:14:30|2790.19|C     |\n",
            "|2024-10-01 00:14:42|3481.08|E     |\n",
            "|2024-10-01 00:14:48|1668.96|B     |\n",
            "|2024-10-01 00:14:54|1649.25|A     |\n",
            "|2024-10-01 00:14:36|5949.1 |D     |\n",
            "|2024-10-01 00:14:45|2162.5 |J     |\n",
            "|2024-10-01 00:14:51|1597.5 |F     |\n",
            "+-------------------+-------+------+\n",
            "\n",
            "+-------------------+-------+------+\n",
            "|date               |energy |sensor|\n",
            "+-------------------+-------+------+\n",
            "|2024-10-01 00:14:57|2080.99|G     |\n",
            "|2024-10-01 00:15:00|2682.71|I     |\n",
            "+-------------------+-------+------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=41>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o13.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while calling o47.awaitTermination\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QUESTÃO 1\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Criar a sessão do Spark\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StructuredWebLogExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# JSON de exemplo para inferir o esquema\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "# Criar DataFrame representando o fluxo de entrada\n",
        "try:\n",
        "    # Ler o fluxo do socket\n",
        "    json_lines = spark.readStream.format(\"socket\") \\\n",
        "        .option(\"host\", \"localhost\") \\\n",
        "        .option(\"port\", 7777) \\\n",
        "        .load()\n",
        "\n",
        "    # Analisar o JSON e expandir os campos\n",
        "    json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "        .select(\"json_data.*\")  # Expandir os campos do JSON para colunas\n",
        "\n",
        "    # Variáveis globais para rastrear o estado dos sensores e o total acumulado\n",
        "    global sensor_states\n",
        "    sensor_states = {}\n",
        "    global running_total\n",
        "    running_total = 0.0\n",
        "\n",
        "    # Função para calcular e exibir a soma acumulada\n",
        "    def calculate_running_total(batch_df, epoch_id):\n",
        "        global sensor_states, running_total\n",
        "\n",
        "        # Atualizar o estado e calcular a soma incremental\n",
        "        for row in batch_df.collect():\n",
        "            sensor = row[\"sensor\"]\n",
        "            energy = row[\"energy\"]\n",
        "\n",
        "            # Se o sensor não estiver no estado, inicialize com a primeira leitura\n",
        "            if sensor not in sensor_states:\n",
        "                sensor_states[sensor] = energy\n",
        "                running_total += energy\n",
        "            else:\n",
        "                # Calcular a diferença entre a leitura atual e a última conhecida\n",
        "                energy_difference = energy - sensor_states[sensor]\n",
        "\n",
        "                # Atualizar o total acumulado e o estado do sensor\n",
        "                if energy_difference > 0:  # Garantir que só valores crescentes sejam somados\n",
        "                    running_total += energy_difference\n",
        "                sensor_states[sensor] = energy\n",
        "\n",
        "        # Exibir o total acumulado atualizado\n",
        "        print(f\"Batch {epoch_id} - Running Total Energy: {running_total}\")\n",
        "\n",
        "    # Definir a consulta para processar o fluxo\n",
        "    query = json_lines \\\n",
        "        .writeStream \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .trigger(processingTime='15 seconds') \\\n",
        "        .foreachBatch(calculate_running_total) \\\n",
        "        .start()\n",
        "\n",
        "    query.awaitTermination(60)\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    if 'query' in locals():\n",
        "        query.stop()\n"
      ],
      "metadata": {
        "id": "DgLxGIHsNLkf",
        "outputId": "3c38aed7-d70d-475b-8ea6-b00054608db4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 - Running Total Energy: 0.0\n",
            "Batch 1 - Running Total Energy: 22656.7\n",
            "Batch 2 - Running Total Energy: 28056.179999999993\n",
            "Batch 3 - Running Total Energy: 28056.40999999998\n",
            "Batch 4 - Running Total Energy: 28056.579999999976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title QUESTÃO 2\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, desc, row_number, sum, round\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Iniciar a sessão do Spark\n",
        "spark = SparkSession.builder.appName(\"ProcessFebruaryData\").getOrCreate()\n",
        "\n",
        "# Carregar o CSV de fevereiro\n",
        "readings_february = spark.read.option(\"header\", \"false\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"delimiter\", \";\") \\\n",
        "    .csv(\"february-energy-readings.csv\")\n",
        "\n",
        "# Renomear colunas manualmente (caso o CSV não tenha cabeçalho)\n",
        "readings_february = readings_february.toDF(\"timestamp\", \"sensor\", \"energy\")\n",
        "\n",
        "# Transformar a coluna \"timestamp\" em tipo Date\n",
        "readings_february = readings_february.withColumn(\"date\", to_date(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "# Selecionar a última leitura do dia para cada sensor\n",
        "window_spec = Window.partitionBy(\"sensor\", \"date\").orderBy(desc(\"timestamp\"))\n",
        "daily_last_reading = readings_february.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
        "    .filter(col(\"row_number\") == 1) \\\n",
        "    .drop(\"row_number\")\n",
        "\n",
        "# Calcular o total acumulado diário para os sensores combinados\n",
        "daily_running_total_february = daily_last_reading.groupBy(\"date\").agg(\n",
        "    round(sum(\"energy\"), 2).alias(\"running_total_energy_february\")\n",
        ").orderBy(\"date\")\n",
        "\n",
        "# Declarar a variável global\n",
        "\n",
        "february_totals = {row[\"date\"]: row[\"running_total_energy_february\"] for row in daily_running_total_february.collect()}\n",
        "\n",
        "daily_running_total_february.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "rEzcj5toL_d_",
        "outputId": "3e107095-92c5-430d-be73-5c15ec907bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------------------+\n",
            "|      date|running_total_energy_february|\n",
            "+----------+-----------------------------+\n",
            "|2024-02-01|                      13447.7|\n",
            "|2024-02-02|                      13517.9|\n",
            "|2024-02-09|                      14432.9|\n",
            "|2024-02-10|                      14547.4|\n",
            "|2024-02-11|                      14665.3|\n",
            "|2024-02-12|                      14776.1|\n",
            "|2024-02-13|                      14888.8|\n",
            "|2024-02-14|                      14982.2|\n",
            "|2024-02-15|                      15063.4|\n",
            "|2024-02-16|                      15111.2|\n",
            "|2024-02-18|                      15351.4|\n",
            "|2024-02-19|                      15431.1|\n",
            "|2024-02-20|                      15515.4|\n",
            "|2024-02-21|                      15598.4|\n",
            "|2024-02-22|                      15675.0|\n",
            "|2024-02-23|                      15759.8|\n",
            "|2024-02-24|                     15903.35|\n",
            "|2024-02-25|                     16003.15|\n",
            "|2024-02-26|                     16095.66|\n",
            "|2024-02-27|                     16188.81|\n",
            "+----------+-----------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from datetime import datetime\n",
        "\n",
        "# JSON de exemplo para inferir o esquema\n",
        "sample_json = '{\"date\": \"2024-02-01 00:00:00\", \"sensor\": \"D\", \"energy\": 2615.0}'\n",
        "inferred_schema = schema_of_json(sample_json)\n",
        "\n",
        "# Variáveis globais para rastrear o estado dos sensores e os totais diários\n",
        "sensor_states = {}\n",
        "daily_running_totals = {}\n",
        "current_february_index = 0  # Índice do dia atual no DataFrame de fevereiro\n",
        "\n",
        "# Função para calcular e comparar o total diário\n",
        "def calculate_daily_comparison(batch_df, epoch_id):\n",
        "    global sensor_states, daily_running_totals, current_february_index, daily_running_total_february\n",
        "\n",
        "    # Atualizar o estado e calcular a soma incremental\n",
        "    for row in batch_df.collect():\n",
        "        sensor = row[\"sensor\"]\n",
        "        energy = row[\"energy\"]\n",
        "        timestamp = row[\"date\"]\n",
        "        current_date = datetime.strptime(timestamp.split(\" \")[0], \"%Y-%m-%d\").date()\n",
        "\n",
        "        # Inicializar o total acumulado do dia, se necessário\n",
        "        if current_date not in daily_running_totals:\n",
        "            daily_running_totals[current_date] = 0.0\n",
        "            # Avançar para o próximo dia no DataFrame de fevereiro\n",
        "            current_february_index += 1\n",
        "\n",
        "        # Atualizar o estado de cada sensor\n",
        "        if sensor not in sensor_states:\n",
        "            sensor_states[sensor] = energy\n",
        "            daily_running_totals[current_date] += energy\n",
        "        else:\n",
        "            # Calcular a diferença de energia e atualizar o total diário\n",
        "            energy_difference = energy - sensor_states[sensor]\n",
        "            if energy_difference > 0:  # Garantir que só valores crescentes sejam somados\n",
        "                daily_running_totals[current_date] += energy_difference\n",
        "            sensor_states[sensor] = energy\n",
        "\n",
        "    # Obter o total acumulado diário de fevereiro para comparação\n",
        "    if current_february_index < len(daily_running_total_february.collect()):\n",
        "        february_row = daily_running_total_february.collect()[current_february_index]\n",
        "        february_date = february_row[\"date\"]\n",
        "        february_total = february_row[\"running_total_energy_february\"]\n",
        "\n",
        "        # Comparar com os valores do dia atual na *stream*\n",
        "        for date, total_energy in daily_running_totals.items():\n",
        "            if date == february_date:\n",
        "                percentage = (total_energy / february_total) * 100\n",
        "                print(f\"Date: {date} - Running Total Energy: {total_energy:.2f} KWh - February Percentage: {percentage:.2f}%\")\n",
        "            else:\n",
        "                print(f\"Date: {date} - Running Total Energy: {total_energy:.2f} KWh - No February data available.\")\n",
        "\n",
        "# Criar a sessão do Spark\n",
        "spark = SparkSession.builder.appName(\"StructuredWebLogExample\").getOrCreate()\n",
        "\n",
        "# Criar DataFrame representando o fluxo de entrada\n",
        "try:\n",
        "    # Ler o fluxo do socket\n",
        "    json_lines = spark.readStream.format(\"socket\") \\\n",
        "        .option(\"host\", \"localhost\") \\\n",
        "        .option(\"port\", 7777) \\\n",
        "        .load()\n",
        "\n",
        "    # Analisar o JSON e expandir os campos\n",
        "    json_lines = json_lines.withColumn(\"json_data\", from_json(col(\"value\"), inferred_schema)) \\\n",
        "        .select(\"json_data.*\")\n",
        "\n",
        "    # Definir a consulta para processar o fluxo\n",
        "    query = json_lines \\\n",
        "        .writeStream \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .trigger(processingTime='15 seconds') \\\n",
        "        .foreachBatch(calculate_daily_comparison) \\\n",
        "        .start()\n",
        "\n",
        "    query.awaitTermination(120)\n",
        "except Exception as err:\n",
        "    print(err)\n",
        "    if 'query' in locals():\n",
        "        query.stop()\n"
      ],
      "metadata": {
        "id": "PlZx79WpTFUj",
        "outputId": "3dc57da2-e3e0-4f93-9ee3-ca34fa2c0a4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date: 2024-10-01 - Running Total Energy: 22656.70 KWh - No February data available.\n",
            "Date: 2024-10-01 - Running Total Energy: 28056.31 KWh - No February data available.\n",
            "Date: 2024-10-01 - Running Total Energy: 28056.41 KWh - No February data available.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=41>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o13.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while calling o47.awaitTermination\n"
          ]
        }
      ]
    }
  ]
}